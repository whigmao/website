<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.61.0" />


<title>Bayesian Inference and Decision Theory 2 - Guanghua Mao&#39;s website</title>
<meta property="og:title" content="Bayesian Inference and Decision Theory 2 - Guanghua Mao&#39;s website">


  <link href='/My%20logo.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/My%20logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/categories/">Categories</a></li>
    
    <li><a href="https://github.com/whigmao">GitHub</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">Bayesian Inference and Decision Theory 2</h1>

    
    <span class="article-date">2020-03-02</span>
    

    <div class="article-content">
      


<div id="unit-2-random-variables-parametric-models-and-inference-from-observation" class="section level1">
<h1>Unit 2: Random Variables, Parametric Models, and Inference from Observation</h1>
</div>
<div id="random-variable-and-distribution-functions" class="section level1">
<h1>Random Variable and Distribution Functions</h1>
<p>A random variable is a function from the sample space Ω to a set of outcomes.</p>
<ul>
<li><p><strong>Cumulative distribution function (cdf)</strong> - Value of cdf at <span class="math inline">\(x\)</span> is probability that the random variable is less than or equal to <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Probability mass function (pmf)</strong> - Maps each possible value of a discrete random variable to its probability</p></li>
<li><p><strong>Probability density function (pdf)</strong> - Maps each possible value of a continuous random variable to a positive number representing its likelihood relative to other possible values</p></li>
</ul>
</div>
<div id="central-tendency-and-spread" class="section level1">
<h1>Central Tendency and Spread</h1>
<p>Mean, Median and Mode.</p>
<p>Variance, Standard Deviation, Median absolute deviation adn Credible interval.</p>
</div>
<div id="multivariate-random-variables" class="section level1">
<h1>Multivariate Random Variables</h1>
<p>A joint distribution (also called a multivariate distribution) defines a probability distribution on several random variables.</p>
<p><strong>Pay attention to marginal distribution and conditional distribution.</strong></p>
<ul>
<li><p>The <strong>covariance</strong> matrix measures how random quantities vary together.</p>
<ul>
<li>If <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are independent then <span class="math inline">\(Cov(X_i,X_j) = 0\)</span>.</li>
</ul></li>
<li><p>The <strong>correlation matrix</strong></p>
<ul>
<li>The off-diagonal elements are real numbers between -1 and 1</li>
<li>If RVs are independent then correlation is zero</li>
<li>The closer the correlation is to 1 or -1, the more nearly linearly related the RVs are</li>
</ul></li>
</ul>
</div>
<div id="independence-and-conditional-independence" class="section level1">
<h1>Independence and Conditional Independence</h1>
<p><span class="math inline">\(\underline{\mathbf{X}}\)</span> and <span class="math inline">\(\underline{\mathbf{Y}}\)</span> are <strong>independent</strong> if the conditional distribution of <span class="math inline">\(\underline{\mathbf{X}}\)</span> given <span class="math inline">\(\underline{\mathbf{Y}}\)</span> does not depend on <span class="math inline">\(\underline{\mathbf{Y}}\)</span>.</p>
<p><span class="math inline">\(f(\underline{\mathbf{x}},\underline{\mathbf{y}})=f(\underline{\mathbf{x}}) \cdot f(\underline{\mathbf{y}})\)</span>.</p>
<p><span class="math inline">\(\underline{\mathbf{X}}\)</span> and <span class="math inline">\(\underline{\mathbf{Y}}\)</span> are <strong>conditionally independent</strong> given <span class="math inline">\(\underline{\mathbf{Z}}\)</span> if:</p>
<p><span class="math inline">\(f(\underline{\mathbf{x}}, \underline{\mathbf{y}}, \underline{\mathbf{z}})=f(\underline{\mathbf{x}}|\underline{\mathbf{z}}) \cdot f(\underline{\mathbf{y}}|\underline{\mathbf{z}}) \cdot f(\underline{\mathbf{z}})\)</span></p>
<p>Conditional independence relationships simplify specificiation of the joint distribution.</p>
<hr />
</div>
<div id="parametric-families-of-distributions" class="section level1">
<h1>Parametric Families of Distributions</h1>
<p>Statistics makes use of parametric families of distributions. Many problems are modeled by assuming observations <span class="math inline">\(X_1, …, X_N\)</span> are independent and identically distributed observations from a distribution with gpdf <span class="math inline">\(f(x|unknowm \ paremeter)\)</span>.</p>
<p>There are four steps for the canonical problems:</p>
<ol style="list-style-type: decimal">
<li><p>Use sample to construct an estimate (point or interval) of <span class="math inline">\(unknowm \ paremeter\)</span></p></li>
<li><p>Test a hypothesis about the value of <span class="math inline">\(unknowm \ paremeter\)</span></p></li>
<li><p>Is the functional form <span class="math inline">\(f(x|unknowm \ paremeter)\)</span> an adequate model for the data?</p></li>
<li><p>Predict features (e.g., mean) of a future sample</p></li>
</ol>
<p>Many distributions have multiple parameterizations in common use and it is important not to confuse different parameterizations.</p>
</div>
<div id="distribution-in-r" class="section level1">
<h1>Distribution in R</h1>
<p>In R, there are four functions for each distribution, invoked by adding a prefix to the distribution’s base name:</p>
<ul>
<li><p>p for <strong>probability</strong> – the cumulative distribution function (cdf)</p></li>
<li><p>p for <strong>probability</strong> – the cumulative distribution function (cdf)</p></li>
<li><p>d for <strong>density</strong> – the density or mass function</p></li>
<li><p>r for <strong>random</strong> – generates random numbers from the distribution</p></li>
</ul>
<p>Statistical models often assume the observations are a random sample from a parameterized distribution.</p>
</div>
<div id="is-a-parametric-distribution-a-good-model" class="section level1">
<h1>Is a Parametric Distribution a Good Model?</h1>
<p>Before applying a parametric model, we should assess its adequacy:</p>
<ul>
<li><p>Theoretical assumptions underlying the distribution</p></li>
<li><p>Exploratory data analysis</p></li>
<li><p>Formal goodness-of-fit tests</p></li>
</ul>
<p>A q-q plot is a commonly used diagnostic tool. The plot shows quantiles of data distribution against quantiles of theoretical distribution. If theoretical distribution is correct, the plot should look approximately like the plot of <span class="math inline">\(y=x\)</span>. The problem is what if the paremeters are unknown. The parameters can be estimated from data.</p>
<p>Another diagnostic tool is to compare empirical and theoretical counts for discrete RV.</p>
</div>
<div id="example-modeling-transmission-errors" class="section level1">
<h1>Example: Modeling Transmission Errors</h1>
<p>Number of transmission errors per hour is distributed as Poisson with parameter <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math inline">\(f(x|\lambda)=\frac{e^{-\lambda} \cdot \lambda^x}{x!}\)</span>.</p>
<p>Data on previous system established error rate as 1.6 errors per hour. New system has design goal of cutting this rate in € half to 0.8 errors per hour.</p>
<p>Observe new system for 6 one-hour periods:</p>
<ul>
<li>Data: 1,0,1,2,1,0</li>
</ul>
<p>Have we met the design goal? Does the new system improve the error rate?</p>
<p>We use expert judgment to define prior distribution on a discrete set of values. Experts familiar with the new system design said: “Meeting the design goal of 0.8 errors per hour is about a 50-50 proposition. The chance of making things worse than current rate of 1.6 errors per hour is small but not negligible.” Expert agrees that the discretized distribution shown here is a good reflection of his prior knowledge:</p>
<ul>
<li><p>Expected value is about 1.0</p></li>
<li><p>Distribution is heavy tailed on the right</p></li>
<li><p><span class="math inline">\(P(Λ≤0.8) = 0.54\)</span></p></li>
<li><p><span class="math inline">\(P(Λ≤1.6) = 0.87\)</span></p></li>
<li><p>Values of <span class="math inline">\(Λ\)</span> greater than 3 are unlikely enough to ignore</p></li>
</ul>
<div id="features-of-the-posterior-distribution" class="section level2">
<h2>Features of the Posterior Distribution</h2>
<p>Central tendency:</p>
<ul>
<li><p>Posterior mean of <span class="math inline">\(Λ\)</span> is 0.87</p></li>
<li><p>Prior mean of <span class="math inline">\(Λ\)</span> is 0.97; data mean is .83</p></li>
<li><p>Typically posterior central tendency is a compromise between the prior distribution and the center of the data</p></li>
</ul>
<p>Variation:</p>
<ul>
<li><p>Posterior standard deviation of <span class="math inline">\(Λ\)</span> is about 0.33</p></li>
<li><p>Prior standard deviation of <span class="math inline">\(Λ\)</span> is about 0.62</p></li>
<li><p>Typically variation in the posterior is less than variation in the prior (we have more information)</p></li>
</ul>
<p>Meeting the threshold</p>
<ul>
<li><p>Posterior probability of meeting or doing better than design goal is about 0.58</p></li>
<li><p>Posterior probability that new system is better than old system is about 0.96</p></li>
<li><p>Posterior probability that new system is worse than old system is less than 0.02</p></li>
</ul>
</div>
<div id="triplot" class="section level2">
<h2>Triplot</h2>
<p>Triplot is a visual tool for examining Bayesian belief dynamics. It plots prior distribution, normalized likelihood, and posterior distribution.</p>
</div>
<div id="bayesian-belief-dynamics" class="section level2">
<h2>Bayesian Belief Dynamics:</h2>
<p>Batch processing:</p>
<ul>
<li>Use Bayes rule with prior and combined likelihood <span class="math inline">\(f(X_1, …, X_n |\theta)\)</span> to find posterior <span class="math inline">\(f(\theta|X_1, …, X_n )\)</span></li>
</ul>
<p>Sequential processing:</p>
<ul>
<li><p>Use Bayes rule with prior <span class="math inline">\(g(\theta)\)</span> and likelihood <span class="math inline">\(f(X_1|\theta)\)</span> to find posterior <span class="math inline">\(f(\theta|X_1)\)</span></p></li>
<li><p>Use Bayes rule with prior <span class="math inline">\(g(\theta|X_1)\)</span> and likelihood <span class="math inline">\(f(X_2|\theta)\)</span> to find posterior <span class="math inline">\(f(\theta|X_1,X_2)\)</span></p></li>
</ul>
<p>…</p>
<ul>
<li><p>Use Bayes rule with prior <span class="math inline">\(g(\theta|X_1, …, X_{n-1})\)</span> and likelihood <span class="math inline">\(f(X_n|\theta)\)</span> to find posterior <span class="math inline">\(f(\theta|X_1,X_2,…,X_n)\)</span></p></li>
<li><p>The posterior distribution after n observations is the same with both methods</p></li>
</ul>
</div>
</div>
<div id="fundamental-identity-of-bayesian-inference" class="section level1">
<h1>Fundamental Identity of Bayesian Inference</h1>
<div class="figure">
<img src="/post/2020-03-02-bayesian-inference-and-decision-theory-2_files/rrr.png" alt="Byesian Inference" />
<p class="caption">Byesian Inference</p>
</div>
<div id="marginal-likelihood" class="section level2">
<h2>Marginal Likelihood</h2>
<p>Before we have seen <span class="math inline">\(X\)</span>, we use the marginal likelihood to predict the value of <span class="math inline">\(X\)</span>.</p>
<ul>
<li>When used for predicting <span class="math inline">\(X\)</span>, the marginal likelihood is called the predictive distribution for <span class="math inline">\(X\)</span>.</li>
</ul>
<p>After we see <span class="math inline">\(X=x\)</span>, we divide the joint probability <span class="math inline">\(f(x|\theta)g(\theta)\)</span> by the marginal</p>
<p>likelihood <span class="math inline">\(f(x)\)</span> to obtain the posterior probability of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(g(\theta|x)=\frac{f(x|\theta)g(\theta)}{f(x)}\)</span></p>
<ul>
<li>The marginal likelihood <span class="math inline">\(f(x)\)</span> is the normalizing constant in Bayes Rule – we divide by <span class="math inline">\(f(x)\)</span> to ensure that the posterior probabilities sum to 1.</li>
</ul>
</div>
</div>
<div id="bayesian-inference-for-continuous-random-variables" class="section level1">
<h1>Bayesian Inference for Continuous Random Variables</h1>
<p>Inference for continuous random variable is limiting case of inference with discretized random variable as number of bins and width of bin goes to zero.</p>
<ul>
<li><p>Accuracy tends to increase with more bins</p></li>
<li><p>Accuracy tends to increase with smaller area per bin</p></li>
<li><p>Be careful with tail area of unbounded random variables</p></li>
<li><p>Closed form solution for continuous problem exists in special cases (see Unit 3)</p></li>
</ul>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

