---
title: Computer Science illuminated
author: Guanghua Mao
date: '2020-12-06'
slug: '100'
categories:
  - Original Articles
tags:
  - Programming
output:
  blogdown::html_page:
    toc: true
    number_sections: true
---

# Laying the Groundwork

We will examine the different layers that make up a computing system, beginning with the information layer and ending with the communication layer. Our goal is to give you an appreciation and understanding of all aspects of computing systems.

Six layers:

1. Information layer

2. Hardware Layer

3. Programming Layer

4. Operating Systems Layer

5. Applications Layer

6. Communications Layer

---

# Information Layer

Binary values is the way in which computer hardware represents and manages information.

Computers are multimedia devices that manipulate data varying in form from numbers to graphics to video. Because a computer can manipulate only binary values, all forms of data must be represented in binary form. Data is classified as being either continuous (analog) or discrete (digital).

Integer values are represented by their binary equivalent using one of several techniques for representing negative numbers, such as a signed-magnitude or two’s complement. Real numbers are represented by a triple made up of the sign, the digits in the number, and an exponent that specifies the radix point.

A character set is a list of alphanumeric characters and the codes that represent each character. The most commonly used character set today is Unicode (16 bits for each character), which has ASCII as a subset. The 8-bit ASCII set is sufficient for English but not for other (or multiple) languages. There are various ways for compressing text so that it takes less space to store it or less time to transmit it from one machine to another.

Audio information is represented as digitized sound waves. Color is represented by three values that represent the contributions of red, blue, and green, respectively. Two basic techniques are used for representing pictures, bitmaps, and vector graphics. Video is broken up into a series of still images, each of which is represented as a picture.

---

# Hardware Layer

A computer operates at its lowest level by controlling the flow of electricity. Because we are dealing with digital computers that use binary information, we concern ourselves with only two voltage ranges, which we interpret as binary 1 or 0. The flow of electricity is guided by electronic devices called gates, which perform basic logical operations such as NOT, AND, and OR. A gate is created by using one or more transistors, an invention that revolutionized computing.

Gates can be combined into circuits, in which the output of one gate serves as an input to another gate. By designing these circuits carefully, we can create devices that perform more complex tasks such as adding, multiplexing, and storing data. Collections of gates, or complete circuits, are often embedded into a single integrated circuit, or chip, which leads to the concept of a central processing unit (CPU).

The components that make up a computer cover a wide range of devices. Each component has characteristics that dictate how fast, large, and efficient it is. Furthermore, each component plays an integral role in the overall processing of the machine.

The world of computing is filled with jargon and acronyms. The speed of a processor is specified in GHz (gigahertz), the amount of memory is specified in MB (megabytes) and GB (gigabytes), and a display screen is specified in pixels.

The von Neumann architecture is the underlying architecture of most of today’s computers. It has five main parts: memory, the arithmetic/logic (ALU) unit, input devices, output devices, and the control unit. The fetch– execute cycle, under the direction of the control unit, is the heart of the processing. In this cycle, instructions are fetched from memory, decoded, and executed.

RAM and ROM are acronyms for two types of computer memory. RAM stands for random-access memory; ROM stands for read-only memory. The values stored in RAM can be changed; those in ROM cannot.

Secondary storage devices are essential to a computer system. These devices save data when the computer is not running. Magnetic tape, magnetic disk, and flash drives are three common types of secondary storage.

Touch screens are peripheral devices that serve both input and output functions and are appropriate in specific situations such as restaurants and information kiosks. They respond to a human touching the screen with a finger or stylus, and they can determine the location on the screen where the touch occurred. Several touch screen technologies exist, including resistive, capacitive, infrared, and surface acoustic wave (SAW) touch screens. They have varying characteristics that make them appropriate in particular situations.

Although von Neumann machines are by far the most common, other computer architectures have emerged. For example, there are machines with more than one processor so that calculations can be done in parallel, thereby speeding up the processing.

---

# Programming Layer

Now the emphasis changes from what a computer system is to how to use one.

A computer can store, retrieve, and process data. A user can enter data into the machine, and the machine can display data so that the user can see it. At the lowest level of abstraction, instructions to the machine directly relate to these five operations.

A computer’s machine language is the set of instructions that the machine’s hardware is built to recognize and execute. Machine-language programs are written by entering a series of these instructions in their binary form. The Pep/8 is a virtual computer with an A register and two-part instructions. One part of the instruction tells which action the instruction performs, and the other part specifies where the data to be used (if any) can be found. Programs written using the Pep/8 instruction set can be run using a simulator—a program that behaves like the Pep/8 computer.

The Pep/8 assembly language is a language that allows the user to enter mnemonic codes for each instruction rather than binary numbers. Programs written in assembly language are translated into their machine-language equivalents, which are then executed using the Pep/8 simulator.

Pseudocode is a shorthand-like language that people use to express algorithms. It allows the user to name variables (places to put values), input values into variables, and print out the values stored in variables. Pseudocode also allows us to describe algorithms that repeat actions and choose between alternative actions. Asking questions and deferring details are two problem-solving strategies used in algorithm design.

Programs, like algorithms, must be tested. Code coverage testing involves determining the input to the program by looking carefully at the program’s code. Data coverage testing involves determining the input by considering all possible input values.

Computer science is sometimes defined as “the study of algorithms and their efficient implementation in a computer.” The focus of this chapter is on algorithms: their role in problem solving, strategies for developing them, techniques for following and testing them. We choose classic searching and sorting algorithms as the context for the discussion on algorithms.

Because algorithms operate on data, we examine ways to structure data so that it can be more efficiently processed.

Polya, in his classic book *How to Solve It*, outlined a problem-solving strategy for mathematical problems. This strategy can be applied to all problems, including those for which a computer program is to be written. These strategies include asking questions, looking for familiar things, and dividing and conquering; when these strategies are applied, they should lead to a plan for solving a problem. In computing, such a plan is called an algorithm.

Two categories of loops are distinguished: count controlled and event controlled. A count-controlled loop executes the loop a predetermined number of times. An event-controlled loop executes until an event within the loop changes.

Data comes in two forms: atomic (simple) and composite. An array is a homogeneous structure that gives a name to a collection of items and allows the user to access individual items by position within the structure.

Searching is the act of looking for a particular value in an array. In this chapter we examined the linear search in an unsorted array, the linear search in a sorted array, and the binary search in a sorted array. Sorting is the act of putting the items in an array into some kind of order. The selection sort, bubble sort, insertion sort, and Quicksort are four commonly used sorting algorithms.

Recursive algorithms are algorithms for which the name of a subprogram appears in the subprogram itself. The factorial and binary search are naturally recursive algorithms.

In the programming layer, we have moved from the concreteness of machine language, to assembly language, to pseudocode, to expressing algorithms. We then went from algorithms using simple variables to algorithms using arrays.

Now we take a step up in abstraction and talk about abstract containers: composite structures for which we do not know the implementation. In computing circles, these abstract containers are called abstract data types. We know their properties and operations and we understand which types of values they can contain, but we have no information about their internal structure or implementation. That is, we know what the operations are and what they do, but we do not know how the operations are implemented.

The algorithm design we have been using is a top-down model, in which we break a task into smaller pieces. We conclude this chapter with more about subprogram statements, which are both a way to make the code mirror the design and the way that algorithms and subalgorithms communicate.

Lists, stacks, queues, trees, and graphs are all useful abstract composite structures. Each has its own defining property and the operations that guarantee that property. All of these abstract structures include operations to insert items and to remove items. Lists and trees also have operations to find items within the structure.

Lists and trees have the same properties: Items can be inserted, deleted, and retrieved. Items can be inserted in a stack, but the item removed and returned is the last item inserted into the stack—that is, the item that has been in the stack the shortest time. Items can be inserted into a queue, but the item removed and returned is the first item put into the queue—that is, the item that has been in the queue the longest time.

Lists, stack, queues, and trees are merely holding structures, but graphs are more complex. A wealth of mathematical algorithms can be applied to information in a graph. We examined three of these: the breadth-first search, the depth-first search, and the single-source shortest-path search.

Subprogram statements allow subalgorithms to be implemented independently. A subprogram may be value returning, in which case it is called by placing its name and arguments within an expression. Alternatively, a subprogram may be non-value returning (void), in which case the subprogram name is used as a statement in the calling program. Data sent to and from subprograms are transmitted by the use of parameter lists. Parameters may be either reference or value parameters. An argument is passed to a value parameter by sending a copy of the argument to the subprogram. An argument is passed to a reference parameter by sending the address of the argument to the subprogram.

Assembly languages are a step in the right direction, but the programmer still must think in terms of individual machine instructions. To overcome this obstacle, we introduced pseudo code as an informal way to describe algorithms; pseudo code is closer to how humans think and communicate. High-level programming languages are a very formal way of accomplishing the same thing. Because computers can execute only machine code, translators were developed to translate programs written in these high-level languages into machine code.

Object-oriented design focuses on determining the objects within a problem and abstracting (grouping) those objects into classes based on like properties and behaviors. There are four stages to object-oriented decomposition:

* Brainstorming, in which we make a first pass at determining the classes in the problem

* Filtering, in which we review the proposed classes

* Scenarios, in which the responsibilities of each class are determined

* Responsibility algorithms, in which the algorithms are written for each of the responsibilities

An assembler translates an assembly-language program into machine code. A compiler translates a program written in a high-level language either into assembly language (to be later translated into machine code) or into machine code. An interpreter is a program that translates the instructions in a program and executes them immediately. An interpreter does not output machine-language code.

Various models of high-level programming languages exist, classified as either imperative (procedural and object-oriented) or declarative (functional and logic). The imperative model describes the processing to be done. The declarative model describes what is to be done, not how it is to be accomplished. The procedural model is based on the concept of a hierarchy of tasks to be completed; the object-oriented model is based on the concept of interacting objects. The functional model is based on the mathematical concept of a function; the logic model is based on mathematical logic.

A Boolean expression is an assertion about the state of a program. Boolean expressions are used to allow a program to execute one section of code or another (conditional statements) and to repeat a section of code (looping statements).

Each variable in a program is a certain data type. Strong typing means that variables are given a type and that only values of that data type can be stored into the variable. Storing a value into a variable is called assigning the value to the variable (assignment statements).

Object-oriented programs are characterized by the following constructs:

* Encapsulation, a language feature that enforces information hiding that is implemented using the class construct

* Inheritance, a language feature that allows one class to inherit the properties and behaviors of another class

* Polymorphism, the ability of a language to disambiguate between operations with the same name


---

# Operating System Layer

To understand a computer system, you must understand the software that manages and coordinates its pieces. The operating system of a computer is the glue that holds the hardware and software together. It is the software foundation on which all other software rests, allowing us to write programs that interact with the machine. This chapter and the next one explore the way in which an operating system manages computer resources. Just as a policeman organizes the efficient flow of cars through an intersection, an operating system organizes the efficient flow of programs through a computer system.

An operating system is the part of the system software that manages resources on a computer. It serves as moderator among human users, application software, and the hardware devices in the system.

Multiprogramming is the technique for keeping multiple programs in memory at the same time, contending for time on the CPU. A process is a program in execution. The operating system must perform careful CPU scheduling, memory management, and process management to ensure fair access to the CPU.

Batch processing organizes jobs into batches that use the same or similar resources. Timesharing allows multiple users to interact with a computer at the same time, creating a virtual machine for each user.

An operating system must manage memory to control and monitor where processes are loaded into main memory. Any memory management technique must define the manner in which it binds a logical address to a physical one. Various strategies have been developed for memory management. The single contiguous approach allows only one program other than the operating system to be in main memory. The partition approach divides memory into several partitions into which processes are loaded. Fixed partitions have a set size, whereas dynamic partitions are created to satisfy the unique needs of the processes loaded. Paging divides memory into frames and programs into pages. The pages of a program need not be contiguous in memory. Demand paging allows for only a portion of a program to be in memory at any given time.

An operating system manages a process’s life states, which are the stages a program goes through during its execution. The process control block stores the necessary information for any process.

CPU scheduling algorithms determine which process gets priority to use the CPU next. First-come, first-served CPU scheduling gives priority to the earliest-arriving job. The shortest-job-next algorithm gives priority to jobs with short running times. Round-robin scheduling rotates the CPU among active processes, giving a little time to each process.

Another key resource that the operating system manages is secondary memory—most importantly, magnetic disks. The organization of files and directories on disk plays a pivotal role in everyday computing. Like a card file on a desktop, the file system provides a way to access particular data in a well-organized manner. The directory structure organizes files into categories and subcategories. 

A file system defines the way our secondary memory is organized. A file is a named collection of data with a particular internal structure. Text files are organized as a stream of characters; binary files have a particular format that is meaningful only to applications set up to handle that format.

File types are often indicated by the file extension of the file name. The operating system maintains a list of recognized file types so that it may open them in the correct kind of application and display the appropriate icons in the graphical user interface. The file extension can be associated with any particular kind of application that the user chooses.

The operations performed on files include creating, deleting, opening, and closing files. Of course, files must be able to be read from and written to as well. The operating system provides mechanisms to accomplish the various file operations. In a multiuser system, the operating system must also provide file protection to ensure that only authorized users have access to files.

Directories are used to organize files on disk. They can be nested to form hierarchical tree structures. Path names that specify the location of a particular file or directory can be absolute, originating at the root of the directory tree, or relative, originating at the current working directory.

Disk-scheduling algorithms determine the order in which pending disk requests are processed. First-come, first-served disk scheduling takes all requests in order but is not very efficient. Shortest-seek-time-first disk scheduling is more efficient but could suffer from starvation. SCAN disk scheduling employs the same strategy as an elevator, sweeping from one end of the disk to the other.

---

# Applications Layer

Computers exist to manage and analyze data. Today they affect almost all aspects of our lives. We use general information systems to manage everything from sports statistics to payroll data. Likewise, cash registers and ATMs are supported by large information systems. In this chapter we examine general-purpose software, particularly spreadsheets and database management systems; these help us organize and analyze the huge amounts of data with which we must deal.

An information system is application software that allows the user to organize and manage data. General information system software includes spreadsheets and database management systems. Other domain areas, such as artificial intelligence, have their own specific techniques and support for data management.

A spreadsheet is a software application that sets up a grid of cells to organize data and the formulas used to compute new values. Cells are referenced by their row and column designations, such as A5 or B7. Formulas usually refer to the values in other cells and may rely on built-in functions to compute their result. In addition, formulas may use data across a range of cells. When a formula is stored in a spreadsheet cell, the value computed by the formula is actually shown in the cell. It is important that formulas in a spreadsheet avoid circular references, in which two or more cells rely on one another to compute their results.

Spreadsheets are both versatile and extensible. They can be used in many different situations, and they respond dynamically to change. As values in the spreadsheet change, affected formulas are automatically recalculated to produce updated results. If spreadsheet rows or columns are added, the ranges in spreadsheet formulas are adjusted immediately. Spreadsheets are particularly appropriate for what-if analysis, in which assumptions are modified to see their effect on the rest of the system.

A database management system includes the physical files in which the data are stored, the software that supports access to and modification of that data, and the database schema that specifies the logical layout of the database. The relational model is the most popular database approach today. It is based on organizing data into tables of records (or objects) with particular fields (or attributes). A key field, whose value uniquely identifies individual records in the table, is usually designated for each table.

Relationships among database elements are represented in new tables that may have their own attributes. Relationship tables do not duplicate data in other tables. Instead, they store the key values of the appropriate database records so that the detailed data can be looked up when needed.

Structured Query Language (SQL) is the language used for querying and manipulating relational databases. The select statement is used to formulate queries and has many variations so that particular data can be accessed from the database. Other SQL statements allow data to be added, updated, and deleted from a database.

A database should be carefully designed. Entity-relationship modeling, with its associated ER diagrams, is a popular technique for database design. ER diagrams graphically depict the relationships among database objects and show their attributes and cardinality constraints.

E-commerce is the process of buying and selling services over the Internet. As e-commerce has become increasingly more popular, more stringent security measures have had to be employed to ensure the integrity of sales over the Internet.

The subdiscipline of computing called artificial intelligence (AI) is important in many ways. To many people it represents the future of computing—the evolution of a machine to make it more like a human. To others it is an avenue for applying new and different technologies to problem solving.

The term artificial intelligence probably conjures up various images in your mind, such as a computer playing chess or a robot doing household chores. These are certainly aspects of AI, but it goes far beyond that. AI techniques affect the way we develop many types of application programs, from the mundane to the fantastic. The world of artificial intelligence opens doors that no other aspect of computing does. Its role in the development of state-of-the-art application programs is crucial.

Artificial intelligence deals with the attempts to model and apply the intelligence of the human mind. The Turing test is one measure to determine whether a machine can think like a human by mimicking human conversation.

The discipline of AI has numerous facets. Underlying all of them is the need to represent knowledge in a form that can be processed efficiently. A semantic network is a graphical representation that captures the relationships among objects in the real world. Questions can be answered based on an analysis of the network graph. Search trees are a valuable way to represent the knowledge of adversarial moves, such as in a competitive game. For complicated games like chess, search trees are enormous, so we still have to come up with strategies for efficient analysis of these structures.

An expert system embodies the knowledge of a human expert. It uses a set of rules to define the conditions under which certain conclusions can be drawn. It is useful in many types of decision-making processes, such as medical diagnosis.

Artificial neural networks mimic the processing of the neural networks of the human brain. An artificial neuron produces an output signal based on multiple input signals and the importance we assign to those signals via a weighting system. This mirrors the activity of the human neuron, in which synapses temper the input signals from one neuron to the next.

Natural language processing deals with languages that humans use to communicate, such as English. Synthesizing a spoken voice can be accomplished by mimicking the phonemes of human speech or by replying with prerecorded words. Voice recognition is best accomplished when the spoken words are disjointed, and is even more effective when the system is trained to recognize a particular person’s voiceprint. Comprehending natural language—that is, applying an interpretation to the conversational discourse—lies at the heart of natural language processing. It is complicated by various types of ambiguities that allow one specific sentence to be interpreted in multiple ways.

Robotics, the study of robots, focuses on two categories: fixed robots and mobile robots. Fixed robots stay put and have whatever they are working on come to them. Mobile robots are capable of moving and require the techniques of artificial intelligence to model the environment in which they navigate.

Simulation is a major area of computing that involves building computer models of complex systems and experimenting with those models to observe their results. A model is an abstraction of the real system in which the system is represented by a set of objects or characteristics and the rules that govern their behavior.

There are two major types of simulation: continuous and discrete event. In continuous simulation, changes are expressed in terms of partial differential equations that reflect the relationships among the set of objects or characteristics. In discrete-event simulation, behavior is expressed in terms of entities, attributes, and events, where entities are objects, attributes are characteristics of an entity, and events are interactions among the entities.

Queuing systems are discrete-event simulations in which waiting time is the factor being examined. Random numbers are used to simulate the arrival and duration of events, such as cars at a drive-through bank or customers in a supermarket. Meteorological and seismic models are examples of continuous simulation.

Computer graphics is a fascinating area that combines computers, science, and art. Much of graphics depends on mathematical equations that simulate the natural phenomena presented in the image. Computer graphics combines light interactions, object properties such as transparency and surface texture, object shape, and physical properties to produce images that approach the realism of an actual photograph.

Computer gaming is the simulation of a virtual world within which the players can interact with the system and with each other. A game engine is a software system within which game developers, designers, and programmers create a game’s virtual world.

---

# Communications Layer

For many years, computers have played as important a role in communication as they do in computation. This communication is accomplished using computer networks. Like complex highway systems that connect roads in various ways to allow cars to travel from their origin to their destination, computer networks form an infrastructure that allows data to travel from some source computer to a destination. The computer receiving the data may be around the corner or around the world. This chapter explores some of the details of computer networks.

A network is a collection of computers connected to share resources and data. Network technologies must concern themselves with underlying protocols and data transfer speeds. The client/server model has emerged as an important software technology given our ever-increasing reliance on networks.

Networks are often classified by their scope. A local-area network (LAN) covers a small geographic area and a relatively small number of connected devices. A wide-area network (WAN) embraces the concept of internetworking, connecting one network to another, and covers a large geographic area. A metropolitan-area network (MAN) is specially designed for large cities. LAN topologies include ring, star, and bus networks. Ethernet has become a standard topology for local-area networks.

Open systems are based on a common model of network architecture and protocols, allowing for interoperability. The OSI Reference Model is a seven-layer description of network processing based on open-system principles.

The Internet backbone is a set of high-speed networks provided by various companies. Internet service providers (ISPs) connect to the backbone or to other ISPs and provide connections for both home and business computing. Home connection technologies include phone modems, digital subscriber lines (DSL), and cable modems. Phone modems transfer data as audio signals and, therefore, are quite slow. DSL uses the same phone lines but transfers data digitally. Cable modems are also digital but use cable TV wiring to transfer data.

Messages are transferred over the Internet by breaking them up into packets and sending those packets separately to their destination, where they are reassembled into the original message. Packets may make several intermediate hops between networks before arriving at their destination. Routers are network devices that guide a packet between networks. Repeaters strengthen digital signals before they degrade too much.

Network protocols are layered so that a high-level protocol relies on lower-level protocols that support it. The key lower-level protocol suite for Internet traffic is TCP/IP. IP protocols and software deal with the routing of packets. TCP protocols and software divide messages into packets, reassemble them at the destination, and handle any errors that occur. High-level protocols include SMTP for email traffic, FTP for file transfers, telnet for remote login sessions, and HTTP for web traffic. Several high-level protocols have been assigned port numbers, which are used to help control and process network traffic. MIME types have been defined for many types of documents and special data formats.

A firewall protects a network from inappropriate access and enforces an organization’s access control policy. Some firewalls simply block traffic on specific ports; other, more sophisticated firewalls analyze the content of network traffic.

An Internet network address must pinpoint a particular machine among all possible ones in the world. A hostname uses readable words separated by dots. A hostname is translated into an IP address, which is a numeric address separated into four sections. One part of the IP address identifies the network, and another part identifies the specific host on that network. How the IP address is broken down depends on the network class (A, B, or C) that the address references.

The domain name system (DNS) translates hostnames into IP addresses. DNS has evolved from using a single file containing all of the information into a distributed system dividing the responsibility among millions of domain name servers. Top-level domains, such as .com and .edu, have become crowded, so some new top-level domains, such as .info and .biz, have been approved.

Cloud computing is a service that provides storage space and other resources on the Internet, largely freeing you from the responsibility of managing data and making it available wherever you are. There are various types of cloud services available at varying costs.

Although the terms Internet and Web are often used interchangeably, they are not the same. The World Wide Web is an infrastructure of information distributed among thousands of computers across the world and the software by which that information is accessed. The Web relies on underlying networks, especially the Internet, as the vehicle to exchange the information among users.

A web page contains information as well as references to other resources such as images. A collection of web pages managed by a single person or company is called a website. Links are established among various web pages across the globe, giving credence to the name World Wide Web.

Visiting a website is really the act of requesting that a web page stored on a remote web server be brought to the local computer for viewing. A Uniform Resource Locator (URL) is used to specify the web document the user wishes to view.

Some websites, such as that run by Google, serve as search engines, allowing the user to enter a word or phrase on which to base a search for information. The search engine responds with a list of candidate websites that the user hopes will match his or her needs. Some search engines are based solely on the keywords entered in the search; others try to interpret the concept underlying the search.

Instant messaging (IM) applications have given the Web another level of interaction, allowing users to conduct ongoing conversations online. IM programs are evolving to include graphics and even video.

Weblogs, or blogs, are web-based publications that feature regularly updated articles. The more serious blogs serve as significant resources on particular topics. Others have given rise to “citizen journalists,” whose work supplements that of the mainstream media.

Cookies are small text files that a website deposits on your hard drive, so that when you return to the site, information about you and your prior visit can be incorporated into your current visit. Cookies are often used to track the activities of users, and they are generally considered helpful for both the user and the sites using them. A cookie is not a program, so it cannot execute code on your computer.

Hypertext Markup Language (HTML) is the primary method of defining web pages. An HTML document consists of information that is annotated by tags that specify how a particular element should be treated and formatted. A web browser displays an HTML page without regard to extra spacing, blank lines, or indentation. The tags alone guide the browser, and a given web page may look slightly different when viewed in different browsers.

HTML tags include those that specify the overall document structure as well as tags that perform basic formatting, such as for headings, paragraphs, and centered text. Font styles, such as bold and italics, are specified using tags as well. Unordered and ordered lists have their own sets of tags.

Some HTML tags include attributes that specify additional information. The source attribute of an image tag specifies the file in which the image is stored, for instance. Anchor tags define links and use an attribute to specify the location of the target web page.

There are additional opportunities to interact with and dynamically create the content of web pages. Two technologies that support web-based interaction are Java applets and Java Server Pages (JSPs). Java applets are Java programs designed to be embedded in HTML pages and executed in a web browser. Their cross-platform nature is possible because applets are compiled into Java Bytecode, which is architecture neutral.

Java Server Pages embed scriptlets into HTML code that is executed by the web server to help dynamically define the content of a web page. Scriptlets have all the expressive power of a full language. JSPs are particularly good at coordinating the interaction between a web page and its underlying database.

Extensible Markup Language (XML) is a metalanguage, which means it is used to define other languages. Unlike HTML, whose tags focus on the format of displayed data, XML tags specify the nature of the data. The user is not constrained to use particular tags; he or she can define any tags that make sense for the data being described.

The format and relationships among XML tags are defined in a Document Type Definition (DTD) document. A set of Extensible Stylesheet Language (XSL) transformations define the way the content of an XML document is turned into another format suitable for the current needs of the user.

Social networks are an important part of the way many people interact on the Web. Services like Facebook and LinkedIn facilitate communication among people with similar interests, and they largely eliminate the issue of distance. In general, social networks have a long history, providing a sociological mechanism for studying many kinds of interactions, including the spread of diseases as well as ideas.

Security issues are prevalent throughout all of the layers of a computer system, and some have been explored in previous chapters. This chapter focused on high-level security issues that are faced by almost all users in today’s online world.

Many of these issues fall under the umbrella of information security, which deals with the confidentiality, integrity, and availability of information. Confidentiality ensures that key data remains protected; integrity ensures that data can only be modified by appropriate mechanisms; and availability ensures that authorized users can access the data when needed.

Controlling access to computer systems and software requires user authentication, which uses some kind of user credentials to verify the user’s identity. Authentication credentials could be something the user knows, like a password; something the user has, like a smart card; or a physical trait of the user, such as a fingerprint.

A good password is something you will remember that is difficult for other people to guess. Some systems require particular password criteria, such as a mix of upper- and lowercase letters, numeric digits, and special characters. In general, you shouldn’t share your password with anyone, send it in an email, or use the same password for multiple accounts. Password management software helps keep track of information such as passwords and credit card numbers in a secure manner.

There are other techniques for controlling access besides username and password control. CAPTCHA is a software mechanism that ensures that the user is a human and not an automated program by having the user type in a string of characters viewed in a distorted image. Fingerprint analysis has become affordable recently and is often used to identify particular uses using special hardware built into a laptop or as a USB peripheral device.

There are several categories of malicious code (or malware) that are designed to cause problems. They include viruses, which infect other software and self-replicate; Trojan horses, which masquerade as beneficial software but have malicious intent; and logic bombs, which are designed to execute in response to a particular system event.

Antivirus software is used to detect, remove, and/or prevent malware. Despite its name, it protects the user from multiple kinds of malware, not just viruses. Antivirus software uses signature detection to identify particular threats, or heuristics that generalize the detection process to include families of similar threats.

Various types of security attacks include password guessing, phishing, and denial-of-service attacks. They also include programming-related problems, such as a programmer deliberately leaving “back door” access to a system and inadvertently creating a potential flaw such as a buffer overflow that could leave a user with inappropriately high privileges.

Cryptography is the field of study related to encoding information. Various ciphers can be used to encrypt and decrypt a message. A Caesar cipher and a transposition cipher are two early techniques that are relatively easy to break. Modern cryptography deals with public-key encryption, which has given rise to digital signatures and digital certificates.

The average user should make a better effort to protect their information online. Social network services such as Facebook and Google+ provide mechanisms for determining who can see what in terms of the data you make available, but many users don’t make use of these mechanisms, or don’t even know they exist. Poor decisions regarding online security stems from a false sense of anonymity and assumptions about the security policies of the websites involved.

Protecting data in mobile devices has become a recent problem. Companies like Apple and Google store location data on mobile phones. This data recently has been extracted and used by law enforcement and other sources without permission. This issue is currently in flux and will likely result in new policies and laws.

WikiLeaks is an organization that publishes secret and classified documents on the Web, while protecting the sources of that information from government retaliation. It operates in ways that make it difficult for anyone to influence its practices. While WikiLeaks promotes the need for transparency in a free society, their practices raise concerns, especially in the U.S. military. Furthermore, WikiLeaks was itself recently a target of an online attack that allowed a huge cache of unredacted U.S. State Department documents to be available for download.

---

# Conclusion

Limits are imposed on computer problem-solving by the hardware, the software, and the nature of the problems to be solved. Numbers are infinite, but their representation within a computer is finite. This limitation can cause errors to creep into arithmetic calculations, giving incorrect results. Hardware components can wear out, and information can be lost during intercomputer and intracomputer data transfers.

The sheer size and complexity of large software projects almost guarantee that errors will occur. Testing a program can demonstrate errors, but it cannot prove the absence of errors. The best way to build good software is to pay attention to quality from the first day of the project, applying the principles of software engineering.

Problems vary from those that are very simple to solve to those that cannot be solved at all. Big-O analysis provides a metric that allows us to compare algorithms in terms of the growth rate of a size factor in the algorithm. Polynomial-time algorithms are those algorithms whose Big-O complexity can be expressed as a polynomial in the size factor. Class P problems can be solved with one processor in polynomial time. Class NP problems can be solved in polynomial time with an unlimited number of processors. As proved by Turing, the halting problem does not have a solution.


